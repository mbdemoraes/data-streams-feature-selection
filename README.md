# data-streams-feature-selection

This project can be used as an extension for Massive Online Analysis (MOA) software, and intends to provide a set of the most relevant feature selection algorithms for stream data mining. This work is under construction, but it can be already be used. The complete list of the methods already developed are described below:

## Installation and requirements

MOA Version: v2017.06 (can be download here: https://moa.cms.waikato.ac.nz/downloads/)
WEKA Version: v3.8.2 (can be download here: https://www.cs.waikato.ac.nz/ml/weka/downloading.html)

In order to use this project, please download the file MOA_FeatureSelection.jar from the lib directory. Then, add it to the classpath when launching MOA:

Example (Windows):

> java -cp .;lib/MOA_FeatureSelection.jar;moa.jar;lib/weka.jar -javaagent:sizeofag-1.0.0.jar moa.gui.GUI

Example (Linux/mac):

> java -cp lib/MOA_FeatureSelection.jar:moa.jar:lib/weka.jar -javaagent:sizeofag-1.0.0.jar moa.gui.GUI

## How to use

To use this extension, in MOA's GUI, select the Classification tab. Click in Configure, and at the new window, over Learner, click in Edit. On the "Editing option: learner" window, select "class moa.featureselection.classifiers.NaiveBayes". There will be three options to configure, each one described below:

* numFeatures: number of features the selected feature selection method has to choose.
* fsMethod: which feature selection method will be used to reduce the dataset's attributes. They are: 0. None (default); 1. Katakis Method; 2. FBCF; 3. OFS.
* winSize: window size, how much tuples (or instances) will be processed and classified each time.

## Feature Selection Methods 

### Katakis Method

This FS scheme is formed by two steps: a) an incremental feature ranking method, and b) an incremental learning algorithm that can consider a subset of the features during prediction (Naive Bayes). 

*I. Katakis, G. Tsoumakas, I. Vlahavas, Advances in Informatics: 10th Panhellenic Conference on Informatics, PCI 2005, Springer Berlin Heidelberg, 2005, Ch. On the Utility of Incremental Feature Selection for the Classification of Textual Data Streams, pp. 338–348.*

### Fast Correlation-Based Filter (FCBF)

FCBF is a multivariate feature selection method where the class relevance and the dependency between each feature pair are taken into account. Based on information theory, FCBF uses symmetrical uncertainty to calculate dependencies of features and the class relevance. Starting with the full feature set, FCBF heuristically applies a backward selection technique with a sequential search strategy to remove irrelevant and redundant features. The algorithm stops when there are no features left to eliminate.

*H.-L. Nguyen, Y.-K. Woon, W.-K. Ng, L. Wan, Heterogeneous ensemble for feature drifts in data streams, in: Proceedings of the 16th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining - Volume Part II, PAKDD’12, 2012, pp. 1–12.*

### Online Feature Selection (OFS)

OFS proposes an ε-greedy online feature selection method based on weights generated by an online classifier (neural networks) which makes a trade-off between exploration and exploitation of features.

*J. Wang, P. Zhao, S. Hoi, R. Jin, Online feature selection and its applications, IEEE Transactions on Knowledge and Data Engineering 26 (3) (2014) 698–710.*
