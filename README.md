# data-streams-feature-selection

This project can be used as an extension for Massive Online Analysis (MOA) software, and intends to provide a set of the most relevant feature selection algorithms for stream data mining. This work is under construction, but it can be already be used. The complete list of the methods already developed are described below.

## Inspiration publication

Our work was inspired by the excelent publication below, where the authors developed an library for MOA, with different data reduction techniques, including discretization, instance selection and even feature selection methods. 

Sergio Ramírez-Gallego, Bartosz Krawczyk, Salvador García, Michał Woźniak, Francisco Herrera, A survey on Data Preprocessing for Data Stream Mining: Current status and future directions, Neurocomputing, Available online 14 February 2017, ISSN 0925-2312, http://dx.doi.org/10.1016/j.neucom.2017.01.078. (http://www.sciencedirect.com/science/article/pii/S0925231217302631)

## Installation and requirements

MOA Version: v2017.06 (can be download here: https://moa.cms.waikato.ac.nz/downloads/) .
WEKA Version: v3.8.2 (can be download here: https://www.cs.waikato.ac.nz/ml/weka/downloading.html) .

In order to use this project, please download the file MOA_FeatureSelection.jar from the lib directory. Then, add it to the classpath when launching MOA:

Example (Windows):

> java -cp .;lib/MOA_FeatureSelection.jar;moa.jar;lib/weka.jar -javaagent:sizeofag-1.0.0.jar moa.gui.GUI

Example (Linux/mac):

> java -cp lib/MOA_FeatureSelection.jar:moa.jar:lib/weka.jar -javaagent:sizeofag-1.0.0.jar moa.gui.GUI

## How to use

To use this extension, in MOA's GUI, select the Classification tab. Click in Configure, and at the new window, over Learner, click in Edit. On the "Editing option: learner" window, select "class moa.featureselection.classifiers.NaiveBayes". There will be three options to configure, each one described below:

* numFeatures: number of features the selected feature selection method has to choose. Default 10.
* fsMethod: which feature selection method will be used to reduce the dataset's attributes. They are: 0. None (default); 1. Katakis Method; 2. FBCF; 3. OFS.
* winSize: window size, how much tuples (or instances) will be processed and classified each time. Default 1.

## Feature Selection Methods 

### Katakis Method

Katakis et. al. proposes a method for feature selection in online learning by using two steps in conjunction: 1-) an incremental feature ranking method and 2-) an incremental learning algorithm that considers a subet of features during prediction, such as Naive Bayes or k-NN.

*I. Katakis, G. Tsoumakas, I. Vlahavas, Advances in Informatics: 10th Panhellenic Conference on Informatics, PCI 2005, Springer Berlin Heidelberg, 2005, Ch. On the Utility of Incremental Feature Selection for the Classification of Textual Data Streams, pp. 338–348.*

### Online Feature Selection (OFS)

OFS proposes an ε-greedy online supervised feature selection method based on weights generated by an online classifier, which can consider partial input and makes a trade-off between exploration and exploitation of features.

*J. Wang, P. Zhao, S. Hoi, R. Jin, Online feature selection and its applications, IEEE Transactions on Knowledge and Data Engineering 26 (3) (2014) 698–710.*
